{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT MINING for BEGINNER\n",
    "- 본 자료는 텍스트 마이닝을 활용한 연구 및 강의를 위한 목적으로 제작되었습니다.\n",
    "- 본 자료를 강의 목적으로 활용하고자 하시는 경우 꼭 아래 메일주소로 연락주세요.\n",
    "- 본 자료에 대한 허가되지 않은 배포를 금지합니다.\n",
    "- 강의, 저작권, 출판, 특허, 공동저자에 관련해서는 문의 바랍니다.\n",
    "- **Contact : ADMIN(admin@teanaps.com)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAY 10. 영어 텍스트 전처리 실습하기: NLTK\n",
    "- Python의 NLTK 패키지를 활용해 텍스트 데이터를 전처리하는 방법에 대해 다룹니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 영어 텍스트 데이터를 전처리하는 방법 알아보기: NLTK\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. NLTK 패키지 설치하기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 한국어 텍스트 전처리를 도와주는 NLTK 패키지를 설치합니다.\n",
    "\n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. 단어의 원형을 복원하는 첫 번째 방법 알아보기: Stemming\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. cooking : cook\n",
      "2. cookery : cookeri\n",
      "3. believes : believ\n",
      "4. using : use\n"
     ]
    }
   ],
   "source": [
    "# NLTK 패키지에서는 세 가지 Stemming 도구를 제공합니다.\n",
    "# 각 Stemmeer는 Stemming을 하는 방법과 기준이 서로 다릅니다.\n",
    "# 각각의 Stemmer 마다 특성이 다르니 사용해보고 알맞은 Stemmer를 골라 사용해야합니다\n",
    "# 일반적으로 Porter Stemmer가 가장 많이 활용됩니다\n",
    "\n",
    "# 1) Porter Stemmer\n",
    "\n",
    "# Porter Stemmer를 불러옵니다.\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print(\"1. cooking :\", ps.stem(\"cooking\"))\n",
    "print(\"2. cookery :\", ps.stem(\"cookery\"))\n",
    "print(\"3. believes :\", ps.stem(\"believes\"))\n",
    "print(\"4. using :\", ps.stem(\"using\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. cooking : cook\n",
      "2. cookery : cookery\n",
      "3. believes : believ\n",
      "4. using : us\n"
     ]
    }
   ],
   "source": [
    "# 2) Lancaster Stemmer\n",
    "\n",
    "# Lancaster Stemmer를 불러옵니다.\n",
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "print(\"1. cooking :\", ls.stem(\"cooking\"))\n",
    "print(\"2. cookery :\", ls.stem(\"cookery\"))\n",
    "print(\"3. believes :\", ps.stem(\"believes\"))\n",
    "print(\"4. using :\", ls.stem(\"using\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. cooking : cook\n",
      "2. cookery : cookery\n",
      "3. believes : lesidie\n",
      "4. using : us\n"
     ]
    }
   ],
   "source": [
    "# 2) Regexp Stemmer\n",
    "\n",
    "# Regexp Stemmer를 불러옵니다.\n",
    "from nltk.stem import RegexpStemmer\n",
    "rs = RegexpStemmer('ing')\n",
    "\n",
    "print(\"1. cooking :\", rs.stem(\"cooking\"))\n",
    "print(\"2. cookery :\", rs.stem(\"cookery\"))\n",
    "print(\"3. believes :\", rs.stem(\"inglesidie\"))\n",
    "print(\"4. using :\", rs.stem(\"using\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1.3. 단어의 원형을 복원하는 두 번째 방법 알아보기: Lemmatizing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lemmatizing은 사전을 기반으로 단어의 원형을 찾아 복원합니다.\n",
    "# NLTK의 Lemmatizing 기능을 활용하기 위해서는 별도로 사전을 다운로드 받아야 합니다.\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-1. cooking : cooking\n",
      "1-2. cooking(pos=\"v\") : cook\n",
      "1-3. cooking(pos=\"n\") : cooking\n",
      "2. cookbooks : cookbook\n",
      "3. believes : believe\n",
      "4. buses : bus\n",
      "5. using : use\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer를 불러옵니다.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatizer는 lemmatize(WORD, pos=POS) 함수로 호출합니다.\n",
    "# pos 변수에는 v, n, a 세 가지 품사를 입력할 수 있습니다. (필수는 아님)\n",
    "print(\"1-1. cooking :\", lemmatizer.lemmatize(\"cooking\"))\n",
    "print(\"1-2. cooking(pos=\\\"v\\\") :\", lemmatizer.lemmatize(\"cooking\", pos=\"v\"))\n",
    "print(\"1-3. cooking(pos=\\\"n\\\") :\", lemmatizer.lemmatize(\"cooking\", pos=\"n\"))\n",
    "print(\"2. cookbooks :\", lemmatizer.lemmatize(\"cookbooks\"))\n",
    "print(\"3. believes :\", lemmatizer.lemmatize(\"believes\", pos=\"v\"))\n",
    "print(\"4. buses :\", lemmatizer.lemmatize(\"buses\"))\n",
    "print(\"5. using :\", lemmatizer.lemmatize(\"using\", pos=\"v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. 단어의 원형을 복원하는 세 번째 방법 알아보기: Replacer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I should have done that thing I did not do.\n"
     ]
    }
   ],
   "source": [
    "# Replacer를 불러옵니다.\n",
    "from replacers import RegexpReplacer\n",
    "replacer = RegexpReplacer()\n",
    "\n",
    "# replace(SENTENCE) 함수는 문장 내 줄임말을 원래의 형태로 복원합니다.\n",
    "sentence = \"I should've done that thing I didn't do.\"\n",
    "new_sentence = replacer.replace(sentence)\n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1.5. 문단을 문장 단위로, 문장을 단어 단위로 분리하기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello World.', 'How are you?', \"It's good to see you.\", 'Thanks for buying this book.']\n"
     ]
    }
   ],
   "source": [
    "# 1) Sentence Tokenizer\n",
    "# Sentence Tokenizer를 불러옵니다.\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "paragraph = \"Hello World. How are you? It's good to see you. Thanks for buying this book.\"\n",
    "# sent_tokenize(PARAGRAPH) 함수는 다수의 문장이 포함된 문단을 문장 단위로 잘라 리스트로 만듭니다.\n",
    "sentences_list = sent_tokenize(paragraph)\n",
    "print(sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'good', 'to', 'see', 'you']\n"
     ]
    }
   ],
   "source": [
    "# 2) Word Tokenizer\n",
    "# Word Tokenizer를 불러옵니다.\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"It's good to see you\"\n",
    "# word_tokenize(SENTENCE) 함수는 문장을 단어 단위로 잘라 리스트로 만듭니다.\n",
    "word_list = word_tokenize(sentence)\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6. 영문 형태소분석하기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLTK의 형태소분석 기능을 활용하기 위해서는 별도의 다운로드가 필요합니다.\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Text', 'NN'), ('mining', 'NN'), ('is', 'VBZ'), ('difficult', 'JJ'), ('but', 'CC'), ('very', 'RB'), ('valuable', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "word_list = word_tokenize(\"Text mining is difficult but very valuable.\")\n",
    "# NLTK 형태소분석기의 pos_tag(WORD_LIST) 함수의 WORD_LIST에는 문자열(문장)이 아닌 단어단위로 구분된 리스트를 넣어줍니다.\n",
    "word_tagging_list = nltk.tag.pos_tag(word_list)\n",
    "print(word_tagging_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7. 단어 리스트에서 불필요한 단어 제거하기: 불용어 제거\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLTK의 불용어 제거 기능을 활용하기 위해서는 별도로 불용어 사전을 다운로드 받아야 합니다.\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from replacers import RegexpReplacer\n",
    "# 불용어 리스트를 불러옵니다.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 불용어 리스트를 확인합니다.\n",
    "# 불용어 리스트에 포함된 단어는 불용어로 취급하여 제거합니다.\n",
    "english_stop_list = list(stopwords.words('english'))\n",
    "print(english_stop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8. [1.1 ~ 1.7] 과정 활용하기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World. It's good to see you. Thanks for buying this book.\n"
     ]
    }
   ],
   "source": [
    "# 1.1 ~ 1.5 과정을 순서대로 수행합니다.\n",
    "\n",
    "# 1. 원본문단\n",
    "paragraph = \"Hello World. It's good to see you. Thanks for buying this book.\"\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world. it's good to see you. thanks for buying this book.\n"
     ]
    }
   ],
   "source": [
    "# 2. 문자열을 모두 소문자로 변경\n",
    "paragraph = paragraph.lower()\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world. it is good to see you. thanks for buying this book.\n"
     ]
    }
   ],
   "source": [
    "# 3. 문장의 줄임말을 원래의 형태로 복원\n",
    "paragraph = replacer.replace(paragraph)\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello world.', 'it is good to see you.', 'thanks for buying this book.']\n"
     ]
    }
   ],
   "source": [
    "# 4. 여러개 문장이 포함된 문단을 문장 단위로 구분\n",
    "sentence_list = sent_tokenize(paragraph)\n",
    "print(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', '.']\n",
      "['it', 'is', 'good', 'to', 'see', 'you', '.']\n",
      "['thanks', 'for', 'buying', 'this', 'book', '.']\n"
     ]
    }
   ],
   "source": [
    "# 5. 문장을 단어 단위로 구분\n",
    "token_sentence_list = []\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    word_list = word_tokenize(sentence)\n",
    "    token_sentence_list.append(word_list)\n",
    "    print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', '.']\n",
      "['is', 'good', 'see', '.']\n",
      "['thanks', 'buying', 'book', '.']\n"
     ]
    }
   ],
   "source": [
    "# 6-1. 단어 단위로 구분된 문장에서 불용어 제거\n",
    "for token_sentence in token_sentence_list:\n",
    "    for token in token_sentence:\n",
    "        if token in english_stop_list:\n",
    "            token_sentence.remove(token)\n",
    "    print(token_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world']\n",
      "['good', 'see']\n",
      "['thanks', 'buying', 'book']\n"
     ]
    }
   ],
   "source": [
    "# 6-2. 기본으로 제공되는 불용어 외의 단어는 임의로 추가해주면 더 좋을 결과를 얻을 수 있습니다.\n",
    "english_stop_list += [\".\", \"is\"]\n",
    "\n",
    "for token_sentence in token_sentence_list:\n",
    "    for token in token_sentence:\n",
    "        if token in english_stop_list:\n",
    "            token_sentence.remove(token)\n",
    "    print(token_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 'NN'), ('world', 'NN')]\n",
      "[('good', 'JJ'), ('see', 'NN')]\n",
      "[('thanks', 'NNS'), ('buying', 'VBG'), ('book', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# 7. 형태소태깅\n",
    "tag_sentence_list = []\n",
    "\n",
    "for token_sentence in token_sentence_list:        \n",
    "    tag_list = nltk.tag.pos_tag(token_sentence)\n",
    "    tag_sentence_list.append(tag_list)\n",
    "    print(tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world']\n",
      "['good', 'see']\n",
      "['thank', 'buy', 'book']\n"
     ]
    }
   ],
   "source": [
    "# 8-1. 원형복원: Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for tag_sentence in tag_sentence_list:\n",
    "    print([ps.stem(word) for word, pos in tag_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world']\n",
      "['good', 'see']\n",
      "['thanks', 'buy', 'book']\n"
     ]
    }
   ],
   "source": [
    "# 8-1. 원형복원: Lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for tag_sentence in tag_sentence_list:\n",
    "    lemma_sentence = []\n",
    "    for word, pos in tag_sentence:\n",
    "        # 품사가 형용사(a), 동사(v), 명사(n)인 경우에만 품사를 옵션으로 넣어줍니다.\n",
    "        if pos.lower()[0] in [\"a\", \"v\", \"n\"]:\n",
    "            lemma_sentence.append(lemmatizer.lemmatize(word, pos=pos.lower()[0]))\n",
    "        else:\n",
    "            lemma_sentence.append(lemmatizer.lemmatize(word))\n",
    "    print(lemma_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
